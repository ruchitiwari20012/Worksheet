Q1:-(a) True

Q2:-(a) Central Limit Theorem

Q3:-(b) Modeling bounded count data

Q4:-(d) All of the mentioned

Q5:-(c) Poisson

Q6:-(b) False

Q7:-(b) Hypothesis

Q8:-(a) 0

Q9:-(c) Outliers cannot conform to the regression relationship

10:- What do you understand by the term Normal Distribution?

Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric 
about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean.
In graph form, normal distribution will appear as a bell curve
The normal distribution model is motivated by the Central Limit Theorem. This theory states that averages
calculated from independent, identically distributed random variables have approximately normal distributions,
regardless of the type of distribution from which the variables are sampled (provided it has finite variance). 


11:- How do you handle missing data? What imputation techniques do you recommend?

There are so many ways though which we can handle the missing data.
1. Deleting Rows
This method commonly used to handle the null values. Here, we either delete a particular row if it has a null
value for a particular feature and a particular column if it has more than 70-75% of missing values. This 
method is advised only when there are enough samples in the data set. One has to make sure that after we have
deleted the data, there is no addition of bias. Removing the data will lead to loss of information which will 
not give the expected results while predicting the output.

2. Replacing With Mean/Median/Mode
This strategy can be applied on a feature which has numeric data like the age of a person or the ticket fare.
We can calculate the mean, median or mode of the feature and replace it with the missing values. This is an
approximation which can add variance to the data set. But the loss of the data can be negated by this method
which yields better results compared to removal of rows and columns. Replacing with the above three 
approximations are a statistical approach of handling the missing values.This method is also called as
leaking the data while training. Another way is to approximate it with the deviation of neighbouring values.
This works better if the data is linear. 

3. Assigning An Unique Category
A categorical feature will have a definite number of possibilities, such as gender, for example. Since they 
have a definite number of classes, we can assign another class for the missing values. Here, the features
Cabin and Embarked have missing values which can be replaced with a new category, say, U for ‘unknown’. 
This strategy will add more information into the dataset which will result in the change of variance. Since 
they are categorical, we need to find one hot encoding to convert it to a numeric form for the algorithm to
understand it.

And there are many more way to handle missing data.

The simplest imputation method is replacing missing values with the mean/median/mode values of the dataset at 
large, or some similar summary statistic. This has the advantage of being the simplest possible approach, and 
one that doesn't introduce any undue bias into the dataset

12:- What is A/B testing.

A/B testing (also known as split testing or bucket testing) is a method of comparing two versions of a webpage
or app against each other to determine which one performs better. AB testing is essentially an experiment where
two or more variants of a page are shown to users at random, and statistical analysis is used to determine which
variation performs better for a given conversion goal.


13:- Is mean imputation of missing data acceptable practice?
In general it is not that much effective but it is good when the data is small
Mean ignore the feature correlation, Mean also reduce the variance of the data
If just estimating means: mean imputation preserves the mean of the observed data
Leads to an underestimate of the standard deviation
Distorts relationships between variables by “pulling” estimates of the correlation toward zero


14:- What is linear regression in statistics?

In statistics, linear regression is a linear approach to modeling the relationship between a scalar response
(or dependent variable) and one or more explanatory variables (or independent variables).The case of one
explanatory variable is called simple linear regression. For more than one explanatory variable, the process 
is called multiple linear regression.
This term is distinct from multivariate linear regression, where
multiple correlated dependent variables are predicted, rather than a single scalar variable.
In linear regression, the relationships are modeled using linear predictor functions whose unknown model
parameters are estimated from the data. Such models are called linear models.
Most commonly, the conditional mean of the response given the values of the explanatory variables 
(or predictors) is assumed to be an affine function of those values; less commonly, the conditional median
or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the 
conditional probability distribution of the response given the values of the predictors, rather than on 
the joint probability distribution of all of these variables, which is the domain of multivariate analysis.

Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively
in practical applications. This is because models which depend linearly on their unknown parameters are easier
to fit than models which are non-linearly related to their parameters and because the statistical properties 
of the resulting estimators are easier to determine.


15. What are the various branches of statistics?

The two branches of statistics are descriptive statistics and inferential statistics. All these branches of 
statistics follow a specific scientific approach which makes them equally essential to every.
Descriptive Statistics
Descriptive statistics is considered as the first part of statistical analysis which deals with collection 
and presentation of data. Scientifically, descriptive statistics can be defined as brief explanatory coefficients
that are used by statisticians to summarize a given data set.Generally, a data set can either represent a sample
of a population or the entire populations. Descriptive statistics can be categorized into

•	Measures of central tendency
•	Measures of variability

Inferential Statistics
Inferential statistics are techniques that enable statisticians to use the gathered information from a sample
to make inferences, decisions or predictions about a given population. Inferential statistics often talks in 
probability terms by using descriptive statistics. These techniques are majorly used by statisticians to analyze
data, make estimates and draw conclusions from the limited information which is obtained by sampling and testing 
how reliable the estimates are.
The different types of calculation of inferential statistics include:
•	Regression analysis
•	Analysis of variance (ANOVA)
•	Analysis of covariance (ANCOVA)
•	Statistical significance (t-test)
•	Correlation analysis






