Q1  :- C
Q2  :- C
Q3  :- C
Q4  :- B
Q5  :- D
Q6  :- B
Q7  :- C
Q8  :- C
Q9  :- A,B,AND C 
Q10 :- A,B AND D


Q11 - What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.

Outliers:
A value that lie outside is much smaller or larger than the most of the other values in a set of data are the outliers
Example - In the scores 25,29,3,32,85,33,27,28 form the scores i can see that 3 and 85 both are out of the range so 
they are consider as the outliers
Another Example -  The age of a person may wrongly be recorded as 200 rather than 20 Years. Such an outlier should 
definitely be discarded from the dataset.

Significance of outliers:
•Outliers badly affect mean and standard deviation of the dataset. These may statistically give erroneous results.
•Most machine learning algorithms do not work well in the presence of outlier. So it is desirable to detect and remove
 outliers.
•Outliers are highly useful in anomaly detection like fraud detection where the fraud transactions are very different
 from normal transactions.

Interquartile Range IQR
IQR is used to measure variability by dividing a data set into quartiles.The data is sorted in ascending order and split
into 4 equal parts.Q1, Q2, Q3 called first, second and third quartiles are the values which separate the 4 equal parts.

•Q1 represents the 25th percentile of the data.
•Q2 represents the 50th percentile of the data.
•Q3 represents the 75th percentile of the data.

IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3 – Q1. The data points which fall
below Q1 – 1.5 IQR or above Q3 + 1.5 IQR are outliers.
Example:
Assume the data 6, 2, 1, 5, 4, 3, 50. If these values represent the number of chapatis eaten in lunch,then 50 is clearly
an outlier.


Q12. What is the primary difference between bagging and boosting algorithms?

Differences Between Bagging and Boosting are :–
               BAGGING	                                                   BOOSTING        
1) Simplest way of combining predictions that                  1) A way of combining predictions that
   belong to the same type.                                        belog to the different type
                                     .   
2) Aim to decrease variance, not bias.	                       2) Aim to decrease bias, not variance.

3) Each model receives equal weight.	                       3) Models are weighted according to their performance.

4) Each model is built independently.	                       4) New models are influenced by performance of previously 
                                                                  built models.
5) Different training data subsets are randomly                5) Every new subsets contains the elements that were 
drawn with replacement from the entire training datase.           misclassified by previous models.

6)Bagging tries to solve over-fitting problem.	               6) Boosting tries to reduce bias.

7)If the classifier is unstable (high variance)                7) If the classifier is stable and simple (high bias) 
  then apply bagging.                                            the apply boosting.
	          
Q13 :-What is adjusted R2 in logistic regression. How is it calculated?

Use adjusted R-squared to compare the goodness-of-fit for regression models that contain differing numbers of independent
variables.Let’s say you are comparing a model with five independent variables to a model with one variable and the five
variable model has a higher R-squared. Is the model with five variables actually a better model, or does it just have more
variables? To determine this, just compare the adjusted R-squared values!

The adjusted R-squared adjusts for the number of terms in the model. Importantly, its value increases only when the new 
term improves the model fit more than expected by chance alone. The adjusted R-squared value actually decreases when the 
term doesn’t improve the model fit by a sufficient amount.

So, the simple R squared estimators is upwardly biased. What can we do? Well, we can modify the estimator to try and 
reduce this bias. A number of approaches have been proposed, but the one usually referred to by 'adjusted R squared' 
is motivated by returning to the definition of the population R squared as

R^2 = 1 - ((1 - R^2)(n-1)/(n-k-1))

where R2 is the sample R-square, k is the number of predictors, and n is the total sample size.


Q14. What is the difference between standardisation and normalisation?

The terms normalization and standardization are sometimes used interchangeably,but they usually refer to different
things.Normalization usually means to scale a variable to have a values between 0 and 1,while standardization transforms
data to have a mean of zero and a standard deviation of 1.

Standardisation :-
The result of standardization (or Z-score normalization) is that the features will be rescaled to ensure the mean and 
the standard deviation to be 0 and 1 respectively.This technique is to re-scale features value with the distribution 
value between 0 and 1 is useful for the optimization algorithms,such as gradient descent,that are used within machine 
learning algorithms that weight inputs.Rescaling is also used for algorithms thatuse distance measurements, 
for example, K-Nearest-Neighbours (KNN). 


Normalization :-
Normalization (Min-Max scaling).This technique is to re-scales features with a distribution value between 0 and 1
For every feature,the minimum value of that feature gets transformed into 0, and the maximum value gets transformed into
1. 

However,Normalisation does not treat outliners very well.On the contrary, standardisation allows users to better handle
the outliers and facilitate.

Q15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation. 

In machine learning, we couldn’t fit the model on the training data and can’t say that the model will work accurately for
 the real data. For this, we must assure that our model got the correct patterns from the data, and it is not getting up 
too much noise. For this purpose, we use the cross-validation technique.


Cross-Validation
Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the
complementary subset of the data-set.

The three steps involved in cross-validation are as follows :
*Reserve some portion of sample data-set.
*Using the rest data-set train the model.
*Test the model using the reserve portion of the data-set.

Methods of Cross Validation

Validation
In this method, we perform training on the 50% of the given data-set and rest 50% is used for the testing purpose. The 
major drawback of this method is that we perform training on the 50% of the dataset, it may possible that the remaining 
50% of the data contains some important information which we are leaving while training our model i.e higher bias.

LOOCV (Leave One Out Cross Validation)
In this method, we perform training on the whole data-set but leaves only one data-point of the available data-set and
then iterates for each data-point. It has some advantages as well as disadvantages also.
An advantage of using this method is that we make use of all data points and hence it is low bias.
The major drawback of this method is that it leads to higher variation in the testing model as we are testing against
one data point. If the data point is an outlier it can lead to higher variation. Another drawback is it takes a lot of
execution time as it iterates over ‘the number of data points’ times.

K-Fold Cross Validation
In this method, we split the data-set into k number of subsets(known as folds) then we perform training on the all the
subsets but leave one(k-1) subset for the evaluation of the trained model. In this method, we iterate k times with a
different subset reserved for testing purpose each time.

Advantages of Cross Validation

1. Reduces Overfitting: In Cross Validation, we split the dataset into multiple folds and train the algorithm on different
folds. This prevents our model from overfitting the training dataset. So, in this way, the model attains the generalization
capabilities which is a good sign of a robust algorithm.

Note: Chances of overfitting are less if the dataset is large. So, Cross Validation may not be required at all in the 
situationwhere we have sufficient data available.

2. Hyperparameter Tuning: Cross Validation helps in finding the optimal value of hyperparameters to increase the efficiency
 of the algorithm

Disadvantages of Cross Validation

1. Increases Training Time: Cross Validation drastically increases the training time. Earlier you had to train your model only
 on one training set, but with Cross Validation you have to train your model on multiple training sets. 

For example, if you go with 5 Fold Cross Validation, you need to do 5 rounds of training each on different 4/5 of available data.
 And this is for only one choice of hyperparameters. If you have multiple choice of parameters, then the training period will 
shoot too high.

2. Needs Expensive Computation: Cross Validation is computationally very expensive in terms of processing power required.